{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ece9d83-e338-4bde-baa6-181db89d9e14",
   "metadata": {
    "tags": []
   },
   "source": [
    "# POC - reports generated with GEN AI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d517d7e-1ebf-4d11-80fe-11a1e9910487",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34460eb-484e-45d8-b493-d550c6f4c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb10311-7d05-47de-b817-11e9274d22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "os.environ['HUGGING_FACE_HUB_API_KEY'] = HUGGINGFACEHUB_API_TOKEN #getpass.getpass('Hugging face api key:')\n",
    "\n",
    "\n",
    "#model_id='bigscience/bloom-1b7'\n",
    "#model_id='google/flan-t5-xxl'\n",
    "#model_id='lmsys/fastchat-t5-3b-v1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e921b-256c-4594-9b77-ce0e4a3210cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc696a97-4ca7-4df7-978a-38c6a5ee7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model used to translate the text\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/translation-en-pt-t5\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/translation-en-pt-t5\", max_length=1023)\n",
    "\n",
    "translate_pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db854e-3899-462d-8ac5-88c5141b579f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the path of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5d26a4-8a38-4fd6-8984-f80dfd867672",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1='[CONECTAM] PCA ETAPA3.pdf'\n",
    "file2='[CONECTAM] PCA ETAPA4.pdf'\n",
    "file3='[CONECTAM] PT.pdf'\n",
    "file4='[CONECTAM] RDA.pdf'\n",
    "source='pdfs/partial_reports/'\n",
    "input_files=[file1,file2,file3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2b8c18e-cc28-487f-b76a-c8c6625f6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(source).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839e9c23-11be-427c-a4c6-a02915f60de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "#system_prompt = \"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the instructions and context provided.\"\n",
    "system_prompt =\"Você é um assistente de perguntas e respostas. Seu objetivo é responder às perguntas com a maior precisão possível, com base nas instruções e no contexto fornecidos.\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93fad767-4150-4a74-92c0-d223b29daf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05425534f9154dca845a520e7f4217d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af094ae1342d4bc49cead81c7fdd0750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a937e175b585465ea7dc8719c17366d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# load the model in HF\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=512,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16 , \"load_in_8bit\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95590ae3-90e2-498e-9ab3-202ef09f78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "\n",
    "#load the Embeddings\n",
    "embed_model = LangchainEmbedding(\n",
    "  HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c520d89d-af5a-4730-990c-c40bf5585b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55dd734f-831d-4da5-a55f-13868736c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc49bf16fef4b1db10a552237f458d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90780807c4e34fa583067e15dafb4f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ff1b903ae4fe9bc9c34e2f889c3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850a107eb28048019dcf9e5e65bd0c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccab85b41c04281b79f3e2ee0c70b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d426b2a7104db6b25d068a4e2a24a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c51b142f88047a69f77825f013da02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f377cac059194cc7aca25832560bc9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbd27772fa741519890cd270d183147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86eb6988-b385-48e7-a31d-ad0e9a08e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6d0f738e224671a81b3a1ce892f4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tema principal dos documentos é o uso do satélite SGDC para a conectividade no Brasil, especificamente para instituições de ensino e comunidades indígenas.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Qual o principal tema dos documentos?\")\n",
    "print(translate_pipeline(f\"translate English to Portuguese:{response}\")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3ece20-5960-4d61-be91-0a70e386bc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5fd868053a437d925dad9bec15cec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A principal justificativa deste projecto consiste em elaborar uma prova de conceito (PoC) para o projecto Plataforma Conectiva Amazônia (PCA), que tem por objectivo proporcionar uma conectividade na Internet de alta velocidade a comunidades remotas da Amazônia. A PoC avaliará a viabilidade de ligação destas comunidades através de tecnologias ópticas por satélite ou fibra, e também testará a arquitectura inicial da plataforma PCA. O projecto será desenvolvido em parceria com a Foxconn, e incluirá a utilização de tecnologias como a geoestatística de satélite e a fibra óptica. A PoC será utilizada para analisar a viabilidade da plataforma PCA, incluindo a conectividade por satélite ou fibra óptica, e para avaliar o desempenho do sistema em diferentes condições. Em termos gerais, a principal justificativa do projecto de PCA é fornecer uma conectividade na Internet de alta velocidade a comunidades remotas da Amazônia. A PoC avaliará a viabilidade da ligação destas comunidades através de tecnologias ópticas por satélite ou fibra óptica, e também irá testar a viabilidade da conexão destas comunidades através de tecnologias ópticas e de fibras ópticas. A PoC será utilizada para avaliar a viabilidade da conexão destas comunidades, incluindo a conectividade por satélite ou fibra óptica, e para avaliar o desempenho do sistema em diferentes condições. A principal justificativa do projecto é fornecer uma conexão na Internet de alta velocidade a comunidades remotas da Amazônia (PCA). A PoC avaliará a viabilidade da conexão destas comunidades através de tecnologias ópticas e fibras ópticas. A PoC será utilizada para avaliar a viabilidade da conexão destas comunidades, incluindo a conectividade por satélite ou fibra óptica, e também para testar a viabilidade da plataforma PCA. A base principal do projecto será a utilização de tecnologias, incluindo a conectividade por satélite ou fibra óptica, e para avaliar o desempenho do sistema em diferentes condições. A principal justificativa do projecto é a de fornecer uma conexão na Internet de alta velocidade a comunidades remotas da Amazônia (PCA). A PoC avaliará a viabilidade da conexão destas comunidades através de tecnologias ópticas e fibras ópticas. A PoC será utilizada para avaliar a viabilidade da plataforma PCA, incluindo a conectividade por satélite ou fibra óptica, e também para avaliar a realização do projecto. A principal justificativa do projecto consiste em testar a viabilidade da plataforma PCA, incluindo a conectividade por satélite ou fibra óptica, e para avaliar o desempenho do sistema em diferentes condições. Em termos globais, a principal justificativa do projecto é fornecer uma conexão na Internet de alta velocidade a comunidades remotas da Amazônia (PCA). A PoC permitirá avaliar a viabilidade da conexão destas comunidades através de tecnologias ópticas e fibras ópticas. A PoC será utilizada para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e também para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PCA, e para avaliar a viabilidade da plataforma PC\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"A partir dos documentos fornecidos elabore a principal justificativa deste projeto\")\n",
    "print(translate_pipeline(f\"translate English to Portuguese:{response}\")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a6e1d-9f15-4a8d-a374-f38c0ffee140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 117.94it/s]\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Faça o resumo de como foi realizada a prova de conceito (POC) do projeto\")\n",
    "print(translate_pipeline(f\"translate English to Portuguese:{response}\")[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa786d8-791a-4e87-9c88-ed2b5c7550d7",
   "metadata": {},
   "source": [
    "### Refs\n",
    "\n",
    "https://github.com/MuhammadMoinFaisal/LargeLanguageModelsProjects/blob/main/Chat_with_Document_LlamaIndex_PaLM2/Chat_with_Documents_LlamaIndex_PaLM2.ipynb\n",
    "\n",
    "https://www.youtube.com/watch?v=aWGn61YT2dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83eb1ac-76e9-405e-bed3-97d1f345a40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
